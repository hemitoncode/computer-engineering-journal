## Date:
Tuesday, November 25

## Peel Census + Fine tuning an AI model

### Introduction:
Today was a breezy day. We had to fill out the Peel Census form, so much of our classtime was taken from us. 

### Learning/Reflections:
Because of that, I decided to talk about fine tuning an AI model today! I have been exploring it (though I am no expert) for my AI app, Next Voters. 

Fine tuning is when you take a pre-trained LLM (large language model => can understand natural language) and you modify the internal configuration through training it with a smaller, but more specalized dataset.

This solves the problem of AI hallucination where the LLM makes up things it does not know. This cannot be possible in a fine tuned model because it adapts itself to understand the dataset which is the task that you wish to complete. 


RAG also solves this problem by feeding the AI with context through a vector db solution. However, fine tuning is needed in scenarios where the output must be standarized. 

In my case, I wanted to build a text classification system which provides a singular class label (economy, immigration, healthcare, etc). 

I gathered my data through news papers which included a text excerpt and its corresponding label.

Then, I used Unsloth, trl, and PEFT which are dependencies needed to fine tune the model. Unsloth loads the model and makes it efficient enough to fine tune on consumer grade hardware (like a MacBook Pro).  

trl allows you to fine tune the model with a specific technique. In our case, PEFT allows us to add LoRA adapters which are needed for trl to modify the configuration of the model (there are other methods for configuring the model though)

<img width="2900" height="1600" alt="image" src="https://github.com/user-attachments/assets/7114a575-aca0-4ee2-8d26-82febe88d7d9" />

### Conclusion:
In conclusion, I did the census form and learned more about fine tuning, though I do not know the technical details surrounding LoRA (which I will later on in my career). 
