## Date:
Tuesday, November 4th

## Vectors + Cosine Similarity in AI

### Introduction:
Today, I worked on some practice problems. Once I was done with those, I decided to do more studying into natural language processing within AI, and I learned more deeply as to what a vector is within the context of word embeddings and cosine similairty.

### Learning/Reflections:
A word embedding is essentially a numerical representation (through a vector which has a size and direction) of the word which is done by an AI model like OpenAI's text-embedding-small or text-embedding-large. Since these models are trained on billions of texts, they know which words are **semantically** the same, meaning which words have the same **MEANING**. Therefore, they make the **directions** of the vector similar for those types of words. For example, the word: immigration and deportation would have about the same direction. It would not be perfectly equal, but close enough.

To calculate the direction, we find the angle using trig (spefically cosine). We do this through a fancy algorithm called **cosine similarity** which will take in 2 vectors (A and B) and then find the direction (angle) associated to it. From that angle, it is able to figure out how similar those 2 angles are which is the value we get back.

Therefore, you can access how close 2 words/phrases are by making them into embeddings (vector) and employing cosine similarity through different coding tools. 


### Cosine Similarity Formula:

<img width="2560" height="1439" alt="image" src="https://github.com/user-attachments/assets/01437ee9-ad3c-4eb9-be06-14c0b12fca49" />

### Conclusion:
In conclusion, I learned about vectors and cosine similarity much better. 
